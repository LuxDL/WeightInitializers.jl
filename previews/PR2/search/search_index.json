{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"WeightInitializers.jl","text":""},{"location":"#weightinitializers","title":"WeightInitializers","text":"<p><code>WeightInitializers.jl</code> provides common weight initialization schemes for deep learning models.</p> <pre><code>using WeightInitializers, Random\n\n# Fixing rng\nrng = Random.MersenneTwister(42)\n\n# Explicit rng call\nweights = kaiming_normal(rng, 2, 5)\n#2\u00d75 Matrix{Float32}:\n# -0.351662   0.0171745   1.12442   -0.296372   -1.67094\n# -0.281053  -0.18941    -0.724099   0.0987538   0.634549\n\n# Default rng call\nweights = kaiming_normal(2, 5)\n#2\u00d75 Matrix{Float32}:\n# -0.227513  -0.265372   0.265788  1.29955  -0.192836\n#  0.687611   0.454679  -0.433656  0.20548   0.292002\n\n# Passing kwargs (if needed) with explicit rng call\nweights_cl = kaiming_normal(rng; gain=1.0)\nweights = weights_cl(rng, 2, 5)\n#2\u00d75 Matrix{Float32}:\n# 0.484056   0.231723   0.164379   0.306147   0.18365\n# 0.0836414  0.666965  -0.396323  -0.711329  -0.382971\n\n# Passing kwargs (if needed) with default rng call\nweights_cl = kaiming_normal(; gain=1.0)\nweights = weights_cl(2, 5)\n#2\u00d75 Matrix{Float32}:\n# -0.160876  -0.187646   0.18794   0.918918  -0.136356\n#  0.486214   0.321506  -0.306641  0.145296   0.206476\n</code></pre> <p></p> <p></p>"},{"location":"#quick-examples","title":"Quick examples","text":"<p>The package is meant to be working with deep learning libraries such as F/Lux. All the methods take as input the chosen <code>rng</code> type and the dimension for the array.</p> <pre><code>weights = init(rng, dims...)\n</code></pre> <p>The <code>rng</code> is optional, if not specified a default one will be used.</p> <pre><code>weights = init(dims...)\n</code></pre> <p>If there is the need to use keyword arguments the methods can be called with just the <code>rng</code> (optionally) and the keywords to get in return a function behaving like the two examples above.</p> <pre><code>weights_init = init(rng; kwargs...)\nweights = weights_init(rng, dims...)\n# or\nweights_init = init(; kwargs...)\nweights = weights_init(dims...)\n</code></pre>"},{"location":"api/","title":"Api","text":""},{"location":"api/#weight-initializers","title":"Weight Initializers","text":"<p># <code>WeightInitializers.zeros32</code> \u2014 Function.</p> <pre><code>zeros32(::AbstractRNG, size...) = zeros(Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of zeros of the given <code>size</code>. (<code>rng</code> is ignored)</p> <p>source</p> <p># <code>WeightInitializers.ones32</code> \u2014 Function.</p> <pre><code>ones32(::AbstractRNG, size...) = ones(Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of ones of the given <code>size</code>. (<code>rng</code> is ignored)</p> <p>source</p> <p># <code>WeightInitializers.rand32</code> \u2014 Function.</p> <pre><code>rand32(rng::AbstractRNG, size...) = rand(rng, Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of random numbers from a uniform distribution of the given <code>size</code>.</p> <p>source</p> <p># <code>WeightInitializers.randn32</code> \u2014 Function.</p> <pre><code>randn32(rng::AbstractRNG, size...) = randn(rng, Float32, size...)\n</code></pre> <p>Return an <code>Array{Float32}</code> of random numbers from a standard normal distribution of the given <code>size</code>.</p> <p>source</p> <p># <code>WeightInitializers.glorot_normal</code> \u2014 Function.</p> <pre><code>glorot_normal(rng::AbstractRNG, size...; gain = 1)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a normal distribution with standard deviation <code>gain * sqrt(2 / (fan_in + fan_out))</code>. This method is described in [1] and also known as Xavier initialization.</p> <p>References</p> <p>[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.</p> <p>source</p> <p># <code>WeightInitializers.glorot_uniform</code> \u2014 Function.</p> <pre><code>glorot_uniform(rng::AbstractRNG, size...; gain = 1)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a uniform distribution on the interval \\([-x, x]\\), where <code>x = gain * sqrt(6 / (fan_in + fan_out))</code>. This method is described in [1] and also known as Xavier initialization.</p> <p>References</p> <p>[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.</p> <p>source</p> <p># <code>WeightInitializers.kaiming_normal</code> \u2014 Function.</p> <pre><code>kaiming_normal(rng::AbstractRNG, size...; gain = \u221a2f0)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers taken from a normal distribution standard deviation <code>gain / sqrt(fan_in)</code></p> <p>References</p> <p>[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.</p> <p>source</p> <p># <code>WeightInitializers.kaiming_uniform</code> \u2014 Function.</p> <pre><code>kaiming_uniform(rng::AbstractRNG, size...; gain = \u221a2f0)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a uniform distribution on the interval <code>[-x, x]</code>, where <code>x = gain * sqrt(3/fan_in)</code>.</p> <p>References</p> <p>[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.</p> <p>source</p> <p># <code>WeightInitializers.truncated_normal</code> \u2014 Function.</p> <pre><code>truncated_normal([rng = default_rng_value()], size...; mean = 0, std = 1, lo = -2, hi = 2)\n</code></pre> <p>Return an <code>Array{Float32}</code> of the given <code>size</code> where each element is drawn from a truncated normal distribution. The numbers are distributed like <code>filter(x -&gt; lo&lt;=x&lt;=hi, mean .+ std .* randn(100))</code>.</p> <p>source</p>"}]}